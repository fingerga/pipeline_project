index_accession = "GCF_000845245.1" #accession number for HCMV reference
samples = ["SRR5660030","SRR5660033","SRR5660044","SRR5660045"] #list of input SRA accession numbers

rule all:
    #making the report file my final output
    input:
        final_out= "PipelineReport.txt"

rule fetch_index:
### this rule downloads the CDs from the reference genome GCF_000845245.1
    output:
        outfile_zip= "ncbi_dataset.zip"
    shell:
        "datasets download genome accession {index_accession} --include cds --filename {output.outfile_zip}"

rule unzip_index:
### this rule unzips the ncbi dataset zip file downloaded in rule fetch_index
    input:
        outfile_zip= "ncbi_dataset.zip"
    output:
        cds= "data/cds_from_genomic.fna"
    shell:
        '''
        unzip -p {input.outfile_zip} > {output.cds}
        rm ncbi_dataset.zip
        '''

rule clean_cds:
### this rule takes in the cds.fna file and cleans the headers; it also creates the cds_report file with # CDs info
    input:
        cds = "data/cds_from_genomic.fna"
    output:
        clean_cds= "data/cds_clean.fna",
        cds_report= "cdsReport.txt"
    shell:
        #first "sed" removes everything after the identifier, second one replaces everything up to "cds_" with a ">"
        #"grep -c ">" {output.clean_cds}" coutns the number of >'s (cds)
        '''
        sed '/^>/s/ .*//' {input.cds} | sed 's/>.*cds_/>/g' > data/cds_clean.fna
        echo "The HCMV genome (GCF_000845245.1) has $(grep -c ">" {output.clean_cds}) CDs" >> {output.cds_report}
        printf "\n" >> {output.cds_report}
        '''

rule build_index:
### this rule builds in the index file for kallisto from the coding sequences from HCMV (downlaoded in rule fetch_index)
    input:
        clean_cds= "data/cds_clean.fna"
    output:
        ref_index= "index.idx"
    shell:
        '''
        kallisto index -i index.idx {input.clean_cds}
        '''

rule kallisto_on_reads:
### this rule runs kallisto on raw read data; takes in the raw reads and reference index generated by rule build_index
### it saves everything to the "quant_reads" directory
    input: 
        r1="sample_data/{sample}/{sample}_1.fastq",
        r2="sample_data/{sample}/{sample}_2.fastq",
        ref_index= "index.idx"
    output:
        quant_reads = directory("quant_reads/{sample}")
    shell:
        # specifying 2 threads so it runs a bit faster
        '''
        kallisto quant -i {input.ref_index} -o {output.quant_reads} -b 10 -t 2 {input.r1} {input.r2}
        '''

rule sleuth:
### this rule runs sleuth on the output from kallisto by calling sleuth_script.R
### and saves the output to the sleuth_report.txt file
    input:
        quant_reads = expand("quant_reads/{sample}",sample=samples),
    output:
        sleuth_report = "sleuthReport.txt"
    shell:
        '''
        Rscript sleuth_script.R 
        cat sleuth_out.txt >> {output.sleuth_report}
        printf "\n" >> {output.sleuth_report}
        rm sleuth_out.txt
        '''

rule fetch_index_gen:
### this rule downloads the genome for the reference genome
    output:
        outfile_zip_gen= "ncbi_dataset_gen.zip"
    shell:
        "datasets download genome accession {index_accession} --include genome --filename {output.outfile_zip_gen}"

rule unzip_index_gen:
### this rule unzips the ncbidataset.zip file and saves just the sequence for the HCMV genome to a fasta file
    input:
        outfile_zip_gen= "ncbi_dataset_gen.zip"
    output:
        ref_genome= "data/HCMVgenome.fasta"
    shell:
        #first "sed" removes everything before the first ">"; second removes everything after "{"
        '''
        unzip -p {input.outfile_zip_gen}  > {output.ref_genome}
        sed -n '/^>/,$p' {output.ref_genome} | sed '/^{{/,$d' > genometemp.fasta
        cat genometemp.fasta > {output.ref_genome}
        rm genometemp.fasta
        rm ncbi_dataset_gen.zip
        '''

rule bowtie_build:
### this rule takes in the HCMV genome and builds the bowtie reference genome
    input:
        ref_genome= "data/HCMVgenome.fasta"
    output: 
        #output file to check that its done before the rule bowtie_run executes
        ref_index_gen= directory("ref_gen")
    shell:
        '''
        mkdir -p ref_gen
        bowtie2-build {input.ref_genome} ref_gen/ref
        '''

rule bowtie_run:
### this rule takes in the raw read data and the indexed reference genome and runs bowtie
### it also generates the bowtie_report files for how many reads were filtered
    input:
        r1="sample_data/{sample}/{sample}_1.fastq",
        r2="sample_data/{sample}/{sample}_2.fastq",
        ref_index_gen= "ref_gen",
        sleuth_report = "sleuthReport.txt"
    output:
        mapped1 = "mapped_reads/{sample}/{sample}.1.fastq",
        mapped2 = "mapped_reads/{sample}/{sample}.2.fastq",
        bowtie_report= "mapped_reads/{sample}/{sample}_report.txt",
    shell:
        #--al-conc outputs any reads that map to the reference to a separate .fastq file (to be used in SPADES assembly)
        #i was running into problems when not making a .sam file, so i had it make one to avoid that problem. 
        # "$(echo $(wc -l < {input.r1}) / 4 | bc)" finds the number of reads in a fastq file (num lines/4)
        '''
        bowtie2 --quiet -x ref_gen/ref -1 {input.r1} -2 {input.r2} --al-conc mapped_reads/{wildcards.sample}/{wildcards.sample}.%.fastq -S {wildcards.sample}.sam
        echo "Sample {wildcards.sample} had $(echo $(wc -l < {input.r1}) / 4 | bc) read pairs before and $(echo $(wc -l < {output.mapped1}) / 4 | bc) read pairs after Bowtie2 filtering." >> {output.bowtie_report}
        '''

rule spades:
### this rule takes the mapped reads from each sample and uses SPADES to assemble it
    input:
        mapped1 = "mapped_reads/{sample}/{sample}.1.fastq",
        mapped2 = "mapped_reads/{sample}/{sample}.2.fastq",
    output:
        assembled_gen= directory("assemblies/{sample}_assembly/"),
    shell:
        # using kmer size 127 (as specified in assignment)
        '''
        spades.py -k 127 -t 2 --only-assembler -1 {input.mapped1} -2 {input.mapped2} -o {output.assembled_gen}
        '''

rule fetch_blast:
### this rule downloads viral genome sequences from the taxon betaherpesvirinae
    output:
        outfile_blast= "ncbi_dataset_blast.zip"
    shell:
        "datasets download virus genome taxon betaherpesvirinae --include genome --filename {output.outfile_blast}"

rule unzip_blast:
### this rule unzips the file from rule fetch_blast and removes extra header/metadata lines
    input:
        outfile_blast= "ncbi_dataset_blast.zip"
    output:
        blast_genome= "data/blastdb.fasta"
    shell:
        #first "sed" removes everything before the first ">"; second removes everything after "The NCBI..."
        '''
        unzip -p {input.outfile_blast}  > {output.blast_genome}
        sed -n '/^>/,$p' {output.blast_genome} | sed '/^The NCBI Datasets Project/,$d' > tempdb.fasta
        cat tempdb.fasta > {output.blast_genome}
        rm tempdb.fasta
        rm ncbi_dataset_blast.zip
        '''

rule build_blast:
### this rule builds a database from a multifasta file (generated in rule fetch_blast and unzip_blast)
    input:
        blast_genome= "data/blastdb.fasta"
    output:
        blast_db = directory("blast_db/")
    shell:
        '''
        makeblastdb -in {input.blast_genome} -out {output.blast_db}/betaherpesvirinae -title betaherpesvirinae -dbtype nucl
        '''

rule longest_contig:
### this rule takes in a spades assembly and runs a python script that finds the longest (first) contig in that assembly
### and outputs it to a new fasta file
    input:
        assembled_gen= "assemblies/{sample}_assembly/"
    output:
        longest_contig = ("assemblies/{sample}_lc.fasta")
    shell:
        #used argparse to specify input and output file name
        '''
        python blast_assembly.py -i {input.assembled_gen}/scaffolds.fasta -o {output.longest_contig}
        '''

rule run_blast:
### this rule takes the longest contig from each sample and blasts it against the betaherpesvirinae database
### and saves output in a tab-delimited file under the directory "blast_out"
    input:
        longest_contig = ("assemblies/{sample}_lc.fasta"),
        blast_db = "blast_db/"
    output:
        blast_out= "blast_out/{sample}"
    shell:
        #running the blastn command with my longest contig file (from rule longest_contig) to the database built in rule build_blast
        #output format 6 for tab delimited + all column names
        #flag -max_hsps 1 makes it so it only reports 1 hit to each subject sequence 
        '''
        blastn -query {input.longest_contig} -db {input.blast_db}/betaherpesvirinae -max_hsps 1 -out {output.blast_out} -outfmt "6 sacc pident length qstart qend sstart send bitscore evalue stitle"
        '''

rule write_report:
### this rule pulls together the other 4 report files and writes to the main PipelineReport.txt file
    input:
        blast_out= expand("blast_out/{sample}",sample= samples),
        bowtie_report= expand("mapped_reads/{sample}/{sample}_report.txt",sample=samples),
        cds_report= "cdsReport.txt",
        sleuth_report = "sleuthReport.txt",
    output:
        final_out= "PipelineReport.txt"
    shell:
        #for the first 3 files, i just used "cat" and redirected the output to my output file
        #for the blast results, i used a for loop to write the headers and each txt file for each sample
        #i havent done a for loop in bash before so i used Google AI to help troubleshoot it
        '''
        cat {input.cds_report} > {output.final_out}
        cat {input.sleuth_report} >> {output.final_out}
        cat {input.bowtie_report} >> {output.final_out}
        echo "" >> {output.final_out}
        for file in {input.blast_out}; do
            sample=$(basename $file)
            echo "$sample:" >> {output.final_out}
            echo "sacc\tpident\tlength\tqstart\tqend\tsstart\tsend\tbitscore\tevalue\tstitle" >> {output.final_out}
            head -n 5 $file >> {output.final_out}
            echo "" >> {output.final_out}
        done
        '''

rule cleanup:
### cleanup rule to remove every file generated by the pipeline for another run
    #QUESTION: should I include this? should i include a cleanup rule that runs after the pipeline is completed to get rid of excess files?
    shell:
        '''
        rm -r quant_reads
        rm -r data
        rm index.idx
        rm PipelineReport.txt
        rm -r mapped_reads
        rm -r ref_gen
        rm -r assemblies
        rm -r blast_db
        rm sleuthReport.txt
        rm cdsReport.txt
        rm -r blast_out
        rm *.sam
        '''
#to do:
    #write README file/documentation